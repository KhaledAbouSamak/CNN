{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 2 part1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SpdDmWPjEdOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Problem 1: Speech Denoising Using 1D CNN**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BGoShWQWFBkE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Implement a 1D CNN that does the speech denoising in the STFT magnitude domain. 1D CNN here means a variant of CNN which does the convolution operation along only one of the axis. In our case it's the frequency axis. **"
      ]
    },
    {
      "metadata": {
        "id": "OZX4OKfFy_Ct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Implementation Approach:**\n",
        "\n",
        "\n",
        "I have constructed 1D CNN with the following structure:\n",
        "\n",
        "\n",
        "*  ** Network Layers:** Input Layer + 2 Convolutional Layers + One Fully Connected Layer + One output  Layer\n",
        "\n",
        "\n",
        "*  ** Input: **The input is shaped to [-1,513,1])\n",
        "\n",
        "*  ** Convolutional layers: ** 2 Convolutional Layers with the following setting:\n",
        "                                              \n",
        "                    - Layer 1: No. of filters = 32, filter size = 16, strides 1 with maxpooling of (2X2) and stride of 2\n",
        "                    - Layer 2: No. of filters = 64, filter size = 8, strides 1 with maxpooling of (2X2) and stride of 2\n",
        "\n",
        "\n",
        "*   **Fully Connected Layer: **2048 nodes\n",
        "*   **Output Layer: **513 nodes\n",
        "\n",
        "*   **Batch Size: **128\n",
        "\n",
        "*   **Number of epochs: **1200\n",
        "\n",
        "\n",
        "*   **Activiation Function:**  Relu activation function in all layers \n",
        "\n",
        "*   **Initializer: **All weights are inialized using He initialization\n",
        "\n",
        "*   **Learning Rate:** 0.0002\n",
        "\n",
        "*   **Optimizer:** Adam Optimizer\n",
        "\n",
        "*   **Loss Function:** Mean Squared Error\n",
        "*   **drop out:** to avoid overfitting dropout rate of 0.4 is used \n",
        "\n",
        "***Results***\n",
        "\n",
        "*   **Loss in training data:**  0.02 to 0.03\n",
        "\n",
        "*   **Calculated SNR for training data:** 18 to 19 dB"
      ]
    },
    {
      "metadata": {
        "id": "jIaGbLOW29LJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the libraries"
      ]
    },
    {
      "metadata": {
        "id": "20hGaBlNDHV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import the needed libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBmX8mUSDJUu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# in colab, you'll need to install this\n",
        "\n",
        "#!pip install librosa \n",
        "import librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qE-N31to5Wcn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import train input and output data"
      ]
    },
    {
      "metadata": {
        "id": "TQeMxowLDJTW",
        "colab_type": "code",
        "outputId": "146bb479-b6db-4716-833f-352001c48e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "s, sr=librosa.load(\"train_clean_male.wav\", sr=None)\n",
        "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
        "\n",
        "\n",
        "sn, sr=librosa.load(\"train_dirty_male.wav\", sr=None)\n",
        "X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
        "\n",
        "X.shape"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 2459)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 397
        }
      ]
    },
    {
      "metadata": {
        "id": "lI9PL_ljDQXv",
        "colab_type": "code",
        "outputId": "d1000a02-adfa-4e73-ae60-c2bc4b798c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Transpose the training data to get the data samples in rows and features in columns and then take the absolute values of the STFT data\n",
        "abs_S = np.abs(S.T)\n",
        "abs_X = np.abs(X.T)\n",
        "\n",
        "print(S.shape)\n",
        "print(X.shape)"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(513, 2459)\n",
            "(513, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vz4GI7E05cWo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import testing data"
      ]
    },
    {
      "metadata": {
        "id": "pVJjNmV3DStV",
        "colab_type": "code",
        "outputId": "1c2f072b-8c06-4749-9d5c-03061d1a68a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "s1, sr=librosa.load('test_x_01.wav', sr=None)\n",
        "S1 =librosa.stft(s1, n_fft=1024, hop_length=512)\n",
        "s2, sr=librosa.load('test_x_02.wav', sr=None)\n",
        "S2 =librosa.stft(s2, n_fft=1024, hop_length=512)\n",
        "\n",
        "\n",
        "#import training data for testing as well\n",
        "s3, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
        "S3 =librosa.stft(s3, n_fft=1024, hop_length=512)\n",
        "print(S1.shape, S2.shape, S3.shape)"
      ],
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(513, 142) (513, 380) (513, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "duYMgg8gUOZj",
        "colab_type": "code",
        "outputId": "349c44d5-fbce-4bed-b83f-a9fc7e6e86d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# import test data\n",
        "test1, sr=librosa.load('test_x_01.wav', sr=None)\n",
        "test1_stft =librosa.stft(test1, n_fft=1024, hop_length=512)\n",
        "\n",
        "test2, sr=librosa.load('test_x_02.wav', sr=None)\n",
        "test2_stft =librosa.stft(test2, n_fft=1024, hop_length=512)\n",
        "\n",
        "\n",
        "\n",
        "#import training data for testing as well\n",
        "training, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
        "training_stft =librosa.stft(training, n_fft=1024, hop_length=512)\n",
        "print(test1_stft.shape, test2_stft.shape, training_stft.shape)"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(513, 142) (513, 380) (513, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "96b4gUQhDVKN",
        "colab_type": "code",
        "outputId": "4c635a47-19eb-4bc3-dbcc-18f72bc9d440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(S1.shape, S2.shape, S3.shape)"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(513, 142) (513, 380) (513, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ANOchUvr5kHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Transpose the testing data and then take the absolute values of the STFT data"
      ]
    },
    {
      "metadata": {
        "id": "Id_Pbu0uDbA_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "test1 = np.abs(test1_stft.T)\n",
        "test2 = np.abs(test2_stft.T)\n",
        "training = np.abs(training_stft.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8f39vC3fDn5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The shape of tesing data"
      ]
    },
    {
      "metadata": {
        "id": "kExQjygrDdsP",
        "colab_type": "code",
        "outputId": "24bf977f-0a80-4b1a-e258-dcf733ac4a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print(test1.shape)\n",
        "print(test2.shape)"
      ],
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 513)\n",
            "(380, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dqIqjUa05FNd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create placeholder for the input and ouput variables"
      ]
    },
    {
      "metadata": {
        "id": "IICUHCHQDg1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder('float', shape=(None,513))\n",
        "y = tf.placeholder('float',shape=(None,513))\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WUkMqFZq5vZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Probability for dropouts"
      ]
    },
    {
      "metadata": {
        "id": "RfRmc-O0KhR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keep_prob = tf.placeholder(\"float\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oLrV-j7Z59qR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Set the batch size"
      ]
    },
    {
      "metadata": {
        "id": "hB0rutPdD2Vd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# batch Size\n",
        "batch_size=128\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NVzpt_vDj9d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below Conv1D_CNN function is used to construct the  2 convolutional layers,  one fully connected layer and ouput layer using Relu activation function in all layers including the ouput layer. I have also used He Initialization technique to initialize the weights and bias. In addition, AdamOptimizer is used to optimize the loss. "
      ]
    },
    {
      "metadata": {
        "id": "cavfxtr2DsPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Conv1D_CNN(x, keep_prob):\n",
        "  \n",
        "    # reshape the input, 513 is the width of the input , 1 is the number of channe, -1 is for batch size\n",
        "    input_layer = tf.reshape(x,[-1,513,1])\n",
        "    \n",
        "    \n",
        "    # use He to initialize all weights\n",
        "    initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0 , mode='FAN_IN',uniform=False, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # Convolution layer 1\n",
        "    conv_layer_1 = tf.layers.conv1d(input_layer, \n",
        "                                    filters=16, kernel_size=16, strides=1,\n",
        "                                    padding='same', activation = tf.nn.relu,kernel_initializer =initializer,\n",
        "                                    bias_initializer =initializer) \n",
        "    \n",
        "    #print('çonv1', conv_layer_1.shape)\n",
        "    # max pooling for layer 1\n",
        "    \n",
        "    max_pool_1 = tf.layers.max_pooling1d(conv_layer_1, pool_size=2, strides=2, padding='valid')\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # Convolution layer 2\n",
        "    conv_layer_2 = tf.layers.conv1d(inputs=max_pool_1, \n",
        "                                    filters=32, kernel_size=8, strides=1,\n",
        "                                    padding='same', activation = tf.nn.relu, kernel_initializer =initializer, \n",
        "                                    bias_initializer =initializer)\n",
        "      \n",
        "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv_layer_2, pool_size=2, strides=2, padding='valid')\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    conv2_flat = tf.contrib.layers.flatten(max_pool_2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # Fully connected layer\n",
        "    \n",
        "    fc1 = tf.layers.dense(conv2_flat, 2048, activation = tf.nn.relu,kernel_initializer =initializer , bias_initializer =initializer)\n",
        "    \n",
        "    fc1 = tf.nn.dropout(fc1, rate = 1 - keep_prob)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    # Output layer, class prediction\n",
        "    output = tf.layers.dense(fc1, 513,  activation = tf.nn.relu,kernel_initializer =initializer, bias_initializer =initializer )\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5Zm8MKvDwbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The below function is used to train the connected network and calculate the loss and optimize it using Adam optimizer.**\n",
        "\n",
        "mean_squared_error function is used to calculate the loss. However, the model has given loss of around % of 0.03 for the training data"
      ]
    },
    {
      "metadata": {
        "id": "IH0IbquHD9B7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_nets(x):\n",
        "  \n",
        "    prediction = Conv1D_CNN(x, keep_prob)\n",
        "    \n",
        "    cost = tf.losses.mean_squared_error(y,prediction)\n",
        "    \n",
        "    train_step = tf.train.AdamOptimizer(learning_rate= 0.0002).minimize(cost)\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    epochs = 1200\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            start_index = 0\n",
        "            \n",
        "            for _ in range(int(abs_X.shape[0]/batch_size)):\n",
        "                end_index = start_index +batch_size\n",
        "                if end_index > abs_X.shape[0]:\n",
        "                    end_index = abs_X.shape[0]\n",
        "                batch_x = abs_X[start_index:end_index]\n",
        "                batch_y = abs_S[start_index: end_index]\n",
        "                start_index = end_index + 1\n",
        "                _, err = sess.run([train_step, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.6}) \n",
        "\n",
        "                epoch_loss += err\n",
        "            for i in range(epoch % 50 == 0):\n",
        "                print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\n",
        "        print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\n",
        "        \n",
        "       \n",
        "        test1_pred = sess.run(prediction, feed_dict = {x: test1, keep_prob: 1.0})\n",
        "        test2_pred = sess.run(prediction, feed_dict = {x: test2, keep_prob: 1.0})\n",
        "        training_pred = sess.run(prediction, feed_dict = {x: training, keep_prob: 1.0})\n",
        "        \n",
        "        return test1_pred, test2_pred,training_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjmfyFxBEAYV",
        "colab_type": "code",
        "outputId": "2233453e-67d3-400f-8c3d-a9049ec20401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "test1_pred, test2_pred, training_pred= train_nets(x)"
      ],
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0  completed out of  1200 loss:  2.9229963812977076\n",
            "Epoch  50  completed out of  1200 loss:  0.229036383330822\n",
            "Epoch  100  completed out of  1200 loss:  0.15503776725381613\n",
            "Epoch  150  completed out of  1200 loss:  0.12003292376175523\n",
            "Epoch  200  completed out of  1200 loss:  0.10234591574408114\n",
            "Epoch  250  completed out of  1200 loss:  0.08147533680312335\n",
            "Epoch  300  completed out of  1200 loss:  0.07628886087331921\n",
            "Epoch  350  completed out of  1200 loss:  0.06951843132264912\n",
            "Epoch  400  completed out of  1200 loss:  0.065176184871234\n",
            "Epoch  450  completed out of  1200 loss:  0.0698048184858635\n",
            "Epoch  500  completed out of  1200 loss:  0.05055038526188582\n",
            "Epoch  550  completed out of  1200 loss:  0.05727709620259702\n",
            "Epoch  600  completed out of  1200 loss:  0.04638114990666509\n",
            "Epoch  650  completed out of  1200 loss:  0.04630673653446138\n",
            "Epoch  700  completed out of  1200 loss:  0.04518081352580339\n",
            "Epoch  750  completed out of  1200 loss:  0.048253586050122976\n",
            "Epoch  800  completed out of  1200 loss:  0.04110059590311721\n",
            "Epoch  850  completed out of  1200 loss:  0.04282102471916005\n",
            "Epoch  900  completed out of  1200 loss:  0.043710766767617315\n",
            "Epoch  950  completed out of  1200 loss:  0.040953253977932036\n",
            "Epoch  1000  completed out of  1200 loss:  0.03993685852037743\n",
            "Epoch  1050  completed out of  1200 loss:  0.04250715603120625\n",
            "Epoch  1100  completed out of  1200 loss:  0.0382206323556602\n",
            "Epoch  1150  completed out of  1200 loss:  0.040589805925264955\n",
            "Epoch  1199  completed out of  1200 loss:  0.03808011271758005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bBZh_VIZEC1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recover the time domain speech signal\n",
        "out1 = test1_pred.T * (S1/test1.T)\n",
        "\n",
        "\n",
        "out2 = (S2/test2.T)* test2_pred.T\n",
        "out3 = (S3/training.T)* training_pred.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYp3-0WsEFgK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# apply STFT\n",
        "test1_recons = librosa.istft(out1, win_length= 1024, hop_length=512)\n",
        "test2_recons = librosa.istft(out2, win_length= 1024, hop_length=512)\n",
        "test3_recons = librosa.istft(out3, win_length= 1024, hop_length=512)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4XS6GhSEFeu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# write it out\n",
        "\n",
        "librosa.output.write_wav('test_s_01_recons.wav', test1_recons, sr)\n",
        "librosa.output.write_wav('test_s_02_recons.wav', test2_recons, sr)\n",
        "librosa.output.write_wav('training_s_03_recons.wav', test3_recons, sr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qodhl7wKJglg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Calculate **SNR** for training data"
      ]
    },
    {
      "metadata": {
        "id": "MK3E-KypEC0D",
        "colab_type": "code",
        "outputId": "2d8f048d-412d-464c-8224-824b0e52fe8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Recover the time domain speech signal\n",
        "\n",
        "out3 = training_pred.T * (X/np.abs(X))\n",
        "test3_recons = librosa.istft(out3, win_length= 1024, hop_length=512)\n",
        "s_clean = s[0:test3_recons.size]\n",
        "SNR = 10*np.log10(np.dot(s_clean.T,s_clean)/np.dot((s_clean - test3_recons).T,(s_clean - test3_recons)))\n",
        "SNR"
      ],
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18.893262147903442"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 413
        }
      ]
    },
    {
      "metadata": {
        "id": "1o3D52wIHQAi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "References:\n",
        "\n",
        "\n",
        "*   https://www.tensorflow.org/tutorials/estimators/cnn\n",
        "*   https://stackoverflow.com/questions/38114534/basic-1d-convolution-in-tensorflow\n",
        "*  https://www.datacamp.com/community/tutorials/cnn-tensorflow-python\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}