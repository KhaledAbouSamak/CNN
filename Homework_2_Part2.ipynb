{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_2_Part2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SpdDmWPjEdOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Problem 2: Speech Denoising Using 2D CNN**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BGoShWQWFBkE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Implement a 2D CNN that does the speech denoising in the STFT magnitude domain. 2D CNN here means a variant of CNN which does the convolution operation along two of the axis. It's both the\n",
        "width (frequencies) and the height axes (frames) **"
      ]
    },
    {
      "metadata": {
        "id": "Hl8Npc8K66Ln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Implementation Approach:**\n",
        "\n",
        "\n",
        "I have constructed 2D CNN with the following structure:\n",
        "\n",
        "\n",
        "*  ** Network Layers:** Input Layer + 2 Convolutional Layers + One Fully Connected Layer + One output  Layer\n",
        "\n",
        "\n",
        "*  ** Input: **The input is shaped to [-1, 20, 513,1]\n",
        "\n",
        "*  ** Convolutional layers: ** 2 Convolutional Layers with the following setting:\n",
        "                                              \n",
        "                    - Layer 1: No. of filters = 16, filter size = 4, strides 1 with maxpooling of (2X2) and stride of 2\n",
        "                    - Layer 2: No. of filters = 32, filter size = 2, strides 1 with maxpooling of (2X2) and stride of 2\n",
        "\n",
        "\n",
        "*   **Fully Connected Layer: **2048 nodes\n",
        "\n",
        "*   **Output Layer: **513 nodes\n",
        "\n",
        "*   **Batch Size: **128\n",
        "\n",
        "*   **Number of epochs: **1300\n",
        "\n",
        "\n",
        "*   **Activiation Function:**  Relu activation function in all layers \n",
        "\n",
        "*   **Initializer: **All weights are inialized using He initialization\n",
        "\n",
        "*   **Learning Rate:** 0.0002\n",
        "\n",
        "*   **Optimizer:** Adam Optimizer\n",
        "\n",
        "*   **Loss Function:** Mean Squared Error\n",
        "*   **drop out:** to avoid overfitting dropout rate of 0.4 is used \n",
        "\n",
        "***Results***\n",
        "\n",
        "*   **Loss in training data:**   0.04\n",
        "\n",
        "*   **Calculated the SNR for training data:** 16 to 17 dB"
      ]
    },
    {
      "metadata": {
        "id": "20hGaBlNDHV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import the needed libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBmX8mUSDJUu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# in colab, you'll need to install this\n",
        "\n",
        "#!pip install librosa \n",
        "import librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YGsPUzlgAE8P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the training data"
      ]
    },
    {
      "metadata": {
        "id": "TQeMxowLDJTW",
        "colab_type": "code",
        "outputId": "8b89a26c-1de4-42e0-ff8e-0902d622e991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Import train input and output data\n",
        "s, sr=librosa.load(\"train_clean_male.wav\", sr=None)\n",
        "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
        "\n",
        "\n",
        "sn, sr=librosa.load(\"train_dirty_male.wav\", sr=None)\n",
        "X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
        "\n",
        "\n",
        "\n",
        "# transpose the training data to get the data samples in rows and features in columns and then take the absolute values of the STFT data\n",
        "abs_S = np.abs(S.T)\n",
        "abs_X = np.abs(X.T)\n",
        "\n",
        "print ('train clean', abs_S.shape)\n",
        "print ('train dirty', abs_X.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train clean (2459, 513)\n",
            "train dirty (2459, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yWdff3x8AKXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the testing data"
      ]
    },
    {
      "metadata": {
        "id": "pVJjNmV3DStV",
        "colab_type": "code",
        "outputId": "bcc856ee-0e6a-4db1-9422-59e31f024e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# import test data\n",
        "s1, sr=librosa.load('test_x_01.wav', sr=None)\n",
        "S1 =librosa.stft(s1, n_fft=1024, hop_length=512)\n",
        "s2, sr=librosa.load('test_x_02.wav', sr=None)\n",
        "S2 =librosa.stft(s2, n_fft=1024, hop_length=512)\n",
        "\n",
        "#import training data for testing as well\n",
        "s3, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
        "S3 =librosa.stft(s3, n_fft=1024, hop_length=512)\n",
        "print(S1.shape, S2.shape, S3.shape)\n",
        "\n",
        "training = np.abs(S3.T)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(513, 142) (513, 380) (513, 2459)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zDMRWcuy_Rq1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generate the input as 20 frames shifted by one frame:"
      ]
    },
    {
      "metadata": {
        "id": "gNRBH-dBwbLn",
        "colab_type": "code",
        "outputId": "923b396b-964f-449b-bf6e-f36b50b0dcd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate 2D image for training data\n",
        "training_2D= np.array([np.reshape(abs_S[i:i+20], (20, 513)) for i in range(2440)])\n",
        "\n",
        "\n",
        "print('reshaped training', training_2D.shape)\n",
        "\n",
        "y_= abs_S[19:]\n",
        "\n",
        "print(y_.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reshaped training (2440, 20, 513)\n",
            "(2440, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "duYMgg8gUOZj",
        "colab_type": "code",
        "outputId": "61279609-26d2-4fb6-eabf-448e54cd3d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate 2D image for testing data\n",
        "\n",
        "\n",
        "test1 = np.abs(S1.T)\n",
        "\n",
        "test1_2D = np.array([np.reshape(test1[i:i+20], (20, 513)) for i in range (123)])\n",
        "\n",
        "print('reshaped test1', test1_2D.shape)\n",
        "\n",
        "\n",
        "test2 = np.abs(S2.T)\n",
        "test2_2D = np.array([np.reshape(test2[i:i+20], (20, 513)) for i in range(361)])\n",
        "\n",
        "print('reshaped test2', test2_2D.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reshaped test1 (123, 20, 513)\n",
            "reshaped test2 (361, 20, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2IovUMmd_vOP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create placeholder for the input and ouput variables"
      ]
    },
    {
      "metadata": {
        "id": "IICUHCHQDg1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder('float', shape=(None,20,513))\n",
        "y = tf.placeholder('float',shape=(None,513))\n",
        "\n",
        "# probability for dropouts\n",
        "keep_prob = tf.placeholder(\"float\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YTn7bwHbBXeI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Set the batch size"
      ]
    },
    {
      "metadata": {
        "id": "hB0rutPdD2Vd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# batch Size\n",
        "batch_size=128\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NVzpt_vDj9d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below Conv2D_CNN function is used to construct the network with  2 conolutional layers, one fully comnnected layer and output layer using Relu activation function. I have also used He Initialization technique to initialize the weights and bias. In addition, AdamOptimizer is used to optimize the loss. "
      ]
    },
    {
      "metadata": {
        "id": "cavfxtr2DsPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Conv2D_CNN(x, keep_prob):\n",
        "  \n",
        "    # reshape the input\n",
        "    input_layer = tf.reshape(x, [-1, 20, 513,1])\n",
        "    \n",
        "    \n",
        "    # use He to initialize all weights\n",
        "    initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0 , mode='FAN_IN',uniform=False, dtype=tf.float32)\n",
        "   \n",
        "\n",
        "    \n",
        "\n",
        "    # Convolution layer 1\n",
        "    conv_layer_1 = tf.layers.conv2d(input_layer, \n",
        "                                    filters=16, kernel_size=4, strides=1,\n",
        "                                    padding='same', activation = tf.nn.relu,kernel_initializer =initializer,\n",
        "                                    bias_initializer =initializer) \n",
        "    \n",
        "    max_pool_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=2, strides=2, padding='valid')\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Convolution layer 2\n",
        "    conv_layer_2 = tf.layers.conv2d(inputs=max_pool_1, \n",
        "                                    filters=32, kernel_size=2, strides=1,\n",
        "                                    padding='same', activation = tf.nn.relu, kernel_initializer =initializer, \n",
        "                                    bias_initializer =initializer)\n",
        " \n",
        "\n",
        "    max_pool_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=2, strides=2, padding='valid')\n",
        "   \n",
        "    \n",
        "    \n",
        "    #flatten the output of max pooling layer\n",
        "    \n",
        "    \n",
        "    conv2_flat = tf.contrib.layers.flatten(max_pool_2)\n",
        "    \n",
        "\n",
        "    # Fully connected layer\n",
        "    \n",
        "    fc1 = tf.layers.dense(conv2_flat, 2048 , activation = tf.nn.relu,kernel_initializer =initializer , bias_initializer =initializer)\n",
        "    \n",
        "    fc1 = tf.nn.dropout(fc1,rate = 1 - keep_prob)\n",
        "\n",
        "\n",
        "    # Output layer, class prediction\n",
        "    output = tf.layers.dense(fc1, 513,  activation = tf.nn.relu,kernel_initializer =initializer, bias_initializer =initializer )\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5Zm8MKvDwbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The below function is used to train the connected network and calculate the loss and optimize it using Adam optimizer.**\n",
        "\n",
        "mean_squared_error function is used to calculate the loss. However, the model has given loss of around % of 0.04 for the training data"
      ]
    },
    {
      "metadata": {
        "id": "IH0IbquHD9B7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_nets(x):\n",
        "  \n",
        "    prediction = Conv2D_CNN(x, keep_prob)\n",
        "    \n",
        "    cost = tf.losses.mean_squared_error(y,prediction)\n",
        "    train_step = tf.train.AdamOptimizer(learning_rate= 0.0002).minimize(cost)\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    epochs = 1300\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            start_index = 0\n",
        "            \n",
        "            for _ in range(int(training_2D.shape[0]/batch_size)):\n",
        "                end_index = start_index +batch_size\n",
        "                if end_index > training_2D.shape[0]:\n",
        "                    end_index = training_2D.shape[0]\n",
        "                batch_x = training_2D[start_index:end_index]\n",
        "                batch_y = y_[start_index: end_index]\n",
        "                \n",
        "                start_index = end_index + 1\n",
        "                _, err = sess.run([train_step, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.6}) \n",
        "\n",
        "                epoch_loss += err\n",
        "            for i in range(epoch % 50 == 0):\n",
        "                print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\n",
        "        print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\n",
        "        \n",
        "       \n",
        "        test1_pred = sess.run(prediction, feed_dict = {x: test1_2D, keep_prob: 1.0})\n",
        "        test2_pred = sess.run(prediction, feed_dict = {x: test2_2D, keep_prob: 1.0})\n",
        "        training_pred = sess.run(prediction, feed_dict = {x: training_2D, keep_prob: 1.0})\n",
        "        \n",
        "        return test1_pred, test2_pred,training_pred\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjmfyFxBEAYV",
        "colab_type": "code",
        "outputId": "2e289b79-f7be-451a-d7d2-be99c5703abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "test1_pred, test2_pred, training_pred= train_nets(x)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0  completed out of  1300 loss:  3.889369387179613\n",
            "Epoch  50  completed out of  1300 loss:  0.4967529349960387\n",
            "Epoch  100  completed out of  1300 loss:  0.36544008273631334\n",
            "Epoch  150  completed out of  1300 loss:  0.234587034676224\n",
            "Epoch  200  completed out of  1300 loss:  0.19846920482814312\n",
            "Epoch  250  completed out of  1300 loss:  0.16428576014004648\n",
            "Epoch  300  completed out of  1300 loss:  0.14594825624953955\n",
            "Epoch  350  completed out of  1300 loss:  0.12630866165272892\n",
            "Epoch  400  completed out of  1300 loss:  0.11213923676405102\n",
            "Epoch  450  completed out of  1300 loss:  0.10236282797995955\n",
            "Epoch  500  completed out of  1300 loss:  0.09017294715158641\n",
            "Epoch  550  completed out of  1300 loss:  0.08401874208357185\n",
            "Epoch  600  completed out of  1300 loss:  0.0778257978381589\n",
            "Epoch  650  completed out of  1300 loss:  0.06892852147575468\n",
            "Epoch  700  completed out of  1300 loss:  0.0698070481303148\n",
            "Epoch  750  completed out of  1300 loss:  0.06529376906109974\n",
            "Epoch  800  completed out of  1300 loss:  0.06469096103683114\n",
            "Epoch  850  completed out of  1300 loss:  0.06305202475050464\n",
            "Epoch  900  completed out of  1300 loss:  0.06032890675123781\n",
            "Epoch  950  completed out of  1300 loss:  0.059216897236183286\n",
            "Epoch  1000  completed out of  1300 loss:  0.05798388458788395\n",
            "Epoch  1050  completed out of  1300 loss:  0.05382421979447827\n",
            "Epoch  1100  completed out of  1300 loss:  0.05592178873484954\n",
            "Epoch  1150  completed out of  1300 loss:  0.0540900113992393\n",
            "Epoch  1200  completed out of  1300 loss:  0.052015141234733164\n",
            "Epoch  1250  completed out of  1300 loss:  0.05155412794556469\n",
            "Epoch  1299  completed out of  1300 loss:  0.049645463295746595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UJI2ukvT-JIy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Augment the output  with 19 silent frames"
      ]
    },
    {
      "metadata": {
        "id": "fStP8nQG-Hex",
        "colab_type": "code",
        "outputId": "1f183826-1ab4-4bb8-d614-5db9ac4d4505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate 19 silent frames\n",
        "\n",
        "\n",
        "missing_frames = np.array(np.random.uniform(0,0.1, size = (19,513))/100000)\n",
        "\n",
        "print(missing_frames.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gb145Cr4LQ5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add the silent frames to the predicted data\n",
        "test1_pred= np.vstack((missing_frames, test1_pred))\n",
        "\n",
        "\n",
        "test2_pred= np.vstack((missing_frames, test2_pred))\n",
        "\n",
        "\n",
        "training_pred = np.vstack((missing_frames, training_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBZh_VIZEC1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recover the time domain speech signal\n",
        "out1 = test1_pred.T * (S1/test1.T)\n",
        "\n",
        "\n",
        "out2 = (S2/test2.T)* test2_pred.T\n",
        "out3 = (S3/training.T)* training_pred.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYp3-0WsEFgK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Apply STFT\n",
        "test1_recons = librosa.istft(out1, win_length= 1024, hop_length=512)\n",
        "test2_recons = librosa.istft(out2, win_length= 1024, hop_length=512)\n",
        "test3_recons = librosa.istft(out3, win_length= 1024, hop_length=512)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4XS6GhSEFeu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# write it out\n",
        "\n",
        "librosa.output.write_wav('test_s_01_recons.wav', test1_recons, sr)\n",
        "librosa.output.write_wav('test_s_02_recons.wav', test2_recons, sr)\n",
        "librosa.output.write_wav('training_s_03_recons.wav', test3_recons, sr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OTmFSw47-04m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Calculate  the **SNR** for training data"
      ]
    },
    {
      "metadata": {
        "id": "MK3E-KypEC0D",
        "colab_type": "code",
        "outputId": "8eb40c1d-bf86-455f-e247-bb2ab0575e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# calculate SNR\n",
        "\n",
        "out3 = training_pred.T * (X/np.abs(X))\n",
        "test3_recons = librosa.istft(out3, win_length= 1024, hop_length=512)\n",
        "s_clean = s[0:test3_recons.size]\n",
        "SNR = 10*np.log10(np.dot(s_clean.T,s_clean)/np.dot((s_clean - test3_recons).T,(s_clean - test3_recons)))\n",
        "SNR"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.705366373062134"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "vxrf-l_mzymN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "1.   https://www.datacamp.com/community/tutorials/cnn-tensorflow-python\n",
        "1.   https://www.tensorflow.org/tutorials/estimators/cnn\n",
        "2.   https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}